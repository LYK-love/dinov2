{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to\n",
    "\n",
    "```sh\n",
    "sudo apt install ffmpeg libavcodec-extra\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /scorpio/home/luyukuan/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/scorpio/home/luyukuan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/scorpio/home/luyukuan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/scorpio/home/luyukuan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "# Download model checkpoints:\n",
    "import torch\n",
    "\n",
    "dinov2_vits14_reg_lc = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg_lc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using dinov2 from: /scorpio/home/luyukuan/projects/dinov2/dinov2/__init__.py\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "if 'dinov2' in sys.modules:\n",
    "    del sys.modules['dinov2']\n",
    "    \n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(\"..\")  # Adjust path if your notebook is deeper in directories\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# sys.path.append(project_root)\n",
    "\n",
    "    \n",
    "# Now you can import dinov2\n",
    "import dinov2\n",
    "print(\"Currently using dinov2 from:\", dinov2.__file__)\n",
    "\n",
    "\n",
    "\n",
    "from dinov2.eval.setup import build_model_for_eval\n",
    "from dinov2.configs import load_and_merge_config\n",
    "from dinov2.utils.visualize import *\n",
    "\n",
    "device = \"cuda\"\n",
    "device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "THRESHOLD=0.8\n",
    "TARGET_SIZE = 448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path, model, output_path, threshold=0.6, device='cuda'):\n",
    "    video_prenorm, video_normalized, fps = load_and_preprocess_video(video_path, target_size=TARGET_SIZE, patch_size=model.patch_size)  # 448 is multiple of patch_size (14)\n",
    "    B, C, H, W, patch_size, embedding_dim, patch_num = print_video_model_stats(video_normalized, model)\n",
    "    \n",
    "    patch_embed_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(B), desc=\"Processing Frames\", unit=\"frame\"):\n",
    "            frame = video_normalized[i].unsqueeze(0) # (1, C, H, W)\n",
    "            patch_embed = get_patch_embeddings(model, frame)\n",
    "            patch_embed_list.append(patch_embed)\n",
    "            \n",
    "    patch_embeds = np.vstack(patch_embed_list)  # (B, num_patches, embedding_dim)\n",
    "\n",
    "    # Show progress for PCA processing\n",
    "    print(\"Performing Two-Stage PCA...\")\n",
    "    reduced_embeds, reduced_fg_embeds, nums_of_fg_patches, masks = two_stage_pca(patch_embeds, threshold=threshold)\n",
    "\n",
    "    # Saving video with progress\n",
    "    print(\"Saving output video...\")\n",
    "    save_triple_video(video_prenorm, reduced_embeds, reduced_fg_embeds, nums_of_fg_patches, masks, patch_num, patch_size, output_path=output_path, fps=fps)\n",
    "\n",
    "    print(\"Processing completed! âœ…\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For crane, use b and 0.7.\n",
    "For dog, use b and 0.7.\n",
    "For pong, use b and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 0.00, Total Frames: 0, Duration: 0.00 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m conf \u001b[38;5;241m=\u001b[39m load_and_merge_config(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval/vit\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m14_pretrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model_for_eval(conf, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dinov2/checkpoints/dinov2_vit\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m14_reg4_pretrain.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTHRESHOLD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(video_path, model, output_path, threshold, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m(video_path, model, output_path, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     video_prenorm, video_normalized, fps \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 448 is multiple of patch_size (14)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     B, C, H, W, patch_size, embedding_dim, patch_num \u001b[38;5;241m=\u001b[39m print_video_model_stats(video_normalized, model)\n\u001b[1;32m      5\u001b[0m     patch_embed_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/projects/dinov2/dinov2/utils/visualize.py:74\u001b[0m, in \u001b[0;36mload_and_preprocess_video\u001b[0;34m(video_path, target_size, patch_size, device, hook_function)\u001b[0m\n\u001b[1;32m     72\u001b[0m raw_video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(raw_frames), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Permute to [T, C, H, W] format expected by PyTorch\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m raw_video \u001b[38;5;241m=\u001b[39m \u001b[43mraw_video\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Step 2: Apply hook function to raw video tensor if provided\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "model_size = \"s\"\n",
    "base_dir = \"./data\"\n",
    "# exp_name = \"natural\"\n",
    "# video_name = \"dog_first5sec\"\n",
    "# video_name = \"dog_cat_first5sec\"\n",
    "\n",
    "exp_name = \"pong\"\n",
    "video_name = \"pong_size32\"\n",
    "\n",
    "video_path = f\"{base_dir}/{exp_name}/videos/{video_name}.mp4\"\n",
    "output_path = f\"{base_dir}/{exp_name}/outputs/{video_name}_{model_size}_patch_embed_{THRESHOLD}.mp4\"\n",
    "\n",
    "\n",
    "# Use `dinov2_vitb14_pretrain`\n",
    "conf = load_and_merge_config(f'eval/vit{model_size}14_pretrain')\n",
    "model = build_model_for_eval(conf, f'../dinov2/checkpoints/dinov2_vit{model_size}14_reg4_pretrain.pth')\n",
    "\n",
    "main(video_path, model, output_path, THRESHOLD, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
