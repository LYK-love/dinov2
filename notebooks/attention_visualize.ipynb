{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the last attention layer of DINOv2\n",
    "This is the same visualization method used in DINOv1. I modified the ViT a bit to support the api used in DINOv1, this is inspired by [Tobias Ziegler](https://gitlab.com/ziegleto-machine-learning/dino).\n",
    "\n",
    "Meanwhile, to removes the artifacts, please use model equipped with registers, see this [paper](https://arxiv.org/abs/2309.16588). The artifacts are not observed in DINOv1, whereas it leads to huge impact to DINOv2.\n",
    "\n",
    "\n",
    "The results are still slightly worse compared to DINOv1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the absolute path of the project root\n",
    "project_root = os.path.abspath(\"..\")  # Adjust path if your notebook is deeper in directories\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# We must disable xformers since there's no way to get attention map from xformers attention layers.\n",
    "os.environ['XFORMERS_DISABLED'] = 'True'\n",
    "print(os.environ.get(\"XFORMERS_DISABLED\"))\n",
    "\n",
    "# Now you can import dinov2\n",
    "import dinov2\n",
    "from dinov2.eval.setup import build_model_for_eval\n",
    "from dinov2.configs import load_and_merge_config\n",
    "from dinov2.utils.visualize import *\n",
    "\n",
    "device = \"cuda\"\n",
    "device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path, model, output_path, device='cuda'):\n",
    "    num_register_tokens = 4\n",
    "    raw_tensor, input_tensor, fps = load_preprocess_video(video_path, target_size=448*1, patch_size = model.patch_size)# 448 is multiples of patch_size (14)\n",
    "    B, C, H, W, patch_size, embedding_dim, patch_num = print_video_model_stats(input_tensor, model)\n",
    "    \n",
    "    colorized_attention_map_list = []\n",
    "    last_selfattention_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(B), desc=\"Processing Frames\", unit=\"frame\"):\n",
    "            frame = input_tensor[i].unsqueeze(0)  # Add batch dimension for the model\n",
    "            \n",
    "            \n",
    "            # Forward pass for the single frame\n",
    "            last_selfattention = model.get_last_selfattention(frame).detach().cpu().numpy()\n",
    "            \n",
    "            last_selfattention_list.append(last_selfattention)\n",
    "    \n",
    "    last_selfattentions = np.vstack(last_selfattention_list)  # (B, num_heads, num_tokens, num_tokens)\n",
    "    \n",
    "    # Resize the attention map for the single frame\n",
    "    colorized_attn_maps = attention_visualize(last_selfattentions, height_in_patches = int(H/patch_size), width_in_patches = int(W/patch_size), patch_size=patch_size, num_register_tokens = model.num_register_tokens)\n",
    "            \n",
    "    \n",
    "    save_np_array_as_video(colorized_attn_maps, output_path=output_path, fps=fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 20.00, Total Frames: 448, Duration: 22.40 seconds\n",
      "Input tensor shape: Batch=448, Channels=3, Height=448, Width=448\n",
      "Patch size: 14\n",
      "Embedding dimension: 384\n",
      "Number of patches of each image: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Frames: 100%|██████████| 448/448 [00:13<00:00, 32.74frame/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to ./data/outputs/attn_pong_combined_s.mp4\n"
     ]
    }
   ],
   "source": [
    "# from dinov2.models.vision_transformer import vit_small, vit_large\n",
    "\n",
    "# model = vit_large(\n",
    "#             patch_size=14,\n",
    "#             img_size=526,\n",
    "#             init_values=1.0,\n",
    "#             #ffn_layer=\"mlp\",\n",
    "#             block_chunks=0\n",
    "#     )\n",
    "\n",
    "# model.load_state_dict(torch.load('../dinov2/checkpoints/dinov2_vitl14_pretrain.pth'))\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = False\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "video_path = \"/home/lyk/Projects/dinov2/notebooks/data/videos/crane_video.mp4\"\n",
    "output_path = \"/home/lyk/Projects/dinov2/notebooks/data/outputs/attn_video_crane.mp4\"\n",
    "\n",
    "model_size = \"s\"\n",
    "\n",
    "video_path = \"./data/videos/pong_combined.mp4\"\n",
    "output_path = f\"./data/outputs/attn_pong_combined_{model_size}.mp4\"\n",
    "\n",
    "\n",
    "# Use `dinov2_vitb14_pretrain`\n",
    "conf = load_and_merge_config(f'eval/vit{model_size}14_reg4_pretrain')\n",
    "model = build_model_for_eval(conf, f'../dinov2/checkpoints/dinov2_vit{model_size}14_reg4_pretrain.pth')\n",
    "\n",
    "main(video_path, model, output_path, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
